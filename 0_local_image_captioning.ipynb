{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wti8hYOjlOq",
        "outputId": "64142189-1df1-4292-92dc-7c6f14cd1fc8"
      },
      "id": "4wti8hYOjlOq",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프로젝트 폴더로 경로 이동\n",
        "cd ./drive/MyDrive/INISW_2기/KU_proj/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkAbRWEDjtZf",
        "outputId": "52fd67a2-5f46-4227-875a-bf497b8faf8a"
      },
      "id": "pkAbRWEDjtZf",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/INISW_2기/KU_proj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23831522",
      "metadata": {
        "id": "23831522"
      },
      "source": [
        "# 로컬 개발 코드\n",
        "- 로컬에서 주피터 노트북(Jupyter Notebook), 주피터 랩(JupyterLab) 또는 파이썬(Python)을 이용한다.\n",
        "- 파이토치(pytorch)를 사용하여 딥러닝 프로그램을 개발한다.\n",
        "- 파일명: 0_local_image_captioning.ipynb\n",
        "\n",
        "### 로컬 개발 워크플로우(workflow)  \n",
        "- 로컬 개발 워크플로우를 다음의 4단계로 분리한다.\n",
        "\n",
        "1. 데이터셋 준비(Data Setup)\n",
        "- 로컬 저장소에서 전처리 및 학습에 필요한 학습 데이터셋을 준비한다.\n",
        "\n",
        "2. 데이터 전처리(Data Preprocessing)\n",
        "- 데이터셋의 분석 및 정규화(Normalization)등의 전처리를 수행한다.\n",
        "- 데이터를 모델 학습에 사용할 수 있도록 가공한다.\n",
        "- 추론과정에서 필요한 경우, 데이터 전처리에 사용된 객체를 meta_data 폴더 아래에 저장한다.\n",
        "\n",
        "3. 학습 모델 훈련(Train Model)\n",
        "- 데이터를 훈련에 사용할 수 있도록 가공한 뒤에 학습 모델을 구성한다.\n",
        "- 학습 모델을 준비된 데이터셋으로 훈련시킨다.\n",
        "- 정확도(Accuracy)나 손실(Loss)등 학습 모델의 성능을 검증한다.\n",
        "- 학습 모델의 성능 검증 후, 학습 모델을 배포한다.\n",
        "- 배포할 학습 모델을 meta_data 폴더 아래에 저장한다.\n",
        "\n",
        "4. 추론(Inference)\n",
        "- 저장된 전처리 객체나 학습 모델 객체를 준비한다.\n",
        "- 추론에 필요한 테스트 데이터셋을 준비한다.\n",
        "- 배포된 학습 모델을 통해 테스트 데이터에 대한 추론을 진행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uqgsLn7tJG8w",
      "metadata": {
        "id": "uqgsLn7tJG8w"
      },
      "source": [
        "# 이미지 캡셔닝 (Image Captioning)\n",
        "- 지금부터 이미지 데이터를 이용하여 캡셔닝(captioning)을 진행해보고자 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c067e6a-8187-4727-b55a-436b7d55b780",
      "metadata": {
        "id": "4c067e6a-8187-4727-b55a-436b7d55b780"
      },
      "source": [
        "## 사용할 데이터\n",
        "\n",
        "- AIhub에서 제공하는 Open Dataset인 [유동인구 분석을 위한 cctv 영상 데이터](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=489) 이라는 cctv 영상 데이터셋으로, 총 330 시간의 영상이 존재한다. 우리는 이 중에서 영상 속 등장한 사람 객체만을 크롭(cropp)한 이미지를 기반으로  증강(augmented)한 2만 5천 개의 이미지 데이터셋을 사용하고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 설치(코랩 환경)\n",
        "!pip install transformers==4.27 torch\n",
        "!pip install pycocoevalcap"
      ],
      "metadata": {
        "id": "NnXUQ5f9HYfq"
      },
      "id": "NnXUQ5f9HYfq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "178e030e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "178e030e"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, Swin2SRImageProcessor, Swin2SRForImageSuperResolution\n",
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd1a6259-52f0-4f39-8264-0ba44e9fbd2b",
      "metadata": {
        "id": "fd1a6259-52f0-4f39-8264-0ba44e9fbd2b"
      },
      "source": [
        "## **1. 데이터셋 준비(Data Setup)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2505e67f-d541-4d38-962b-5ee1d0ff11b9",
      "metadata": {
        "tags": [],
        "id": "2505e67f-d541-4d38-962b-5ee1d0ff11b9"
      },
      "outputs": [],
      "source": [
        "# image_augmented.zip 파일 압축 풀기\n",
        "!unzip -qq './image_augmented.zip' -d './meta_data'\n",
        "\n",
        "\n",
        "# 정답 캡션 json 파일 불러오기\n",
        "labelpath=\"./meta_data/annotaions/shuffled_captions.json\"\n",
        "with open(labelpath, 'r',encoding = 'utf-8' or 'cp949' ) as f: # json 파일 접근\n",
        "    captions = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43e25e8-e9de-402a-942e-8a861332ea1d",
      "metadata": {
        "id": "c43e25e8-e9de-402a-942e-8a861332ea1d"
      },
      "source": [
        "## 2. 데이터 전처리 (Data Preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9133e6-5d21-467d-95a7-38b1608fed7b",
      "metadata": {
        "id": "0c9133e6-5d21-467d-95a7-38b1608fed7b"
      },
      "source": [
        "### 데이터 준비 (Preparing Data)\n",
        "\n",
        "앞서 meta_data 폴더에 풀어둔 이미지 데이터와 captions객체에 불러온 정답 캡션을 훈련에 사용할 수 있는 형태로 바꾸고자 한다.\n",
        "\n",
        "- 이미지 데이터 고해상도화(Super Resolution)\n",
        "  - 지나치게 적은 픽셀 수(가로 50, 세로 100)의 이미지의 경우, 고해상도화를 통해 보다 명확한 시각 정보를 담을 수 있게끔 해준다.\n",
        "\n",
        "- 이미지 데이터 정규화 (Normalization)\n",
        "  - preprocessor를 임포트(import)하여 이미지들을 전체 이미지 데이터셋 RGB 값의 평균, 표준편차값을 통해 0 ~ 1 값으로 정규화를 한다.\n",
        "\n",
        "- 데이터 합치기 & 레이블 생성\n",
        "  - 이미지 데이터와 정답 캡션을 매칭하여 최종적으로 모델에 학습할 데이터 집합을 생성한다.\n",
        "\n",
        "- 훈련 (train) & 평가 (val) 데이터셋 생성\n",
        "  - 전체 데이터 중 일부는 훈련 (train)에 사용하고, 나머지 일부는 훈련된 모델의 성능을 평가 (val)하기 위해 사용하고자 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9-4PTTUL4AuP",
      "metadata": {
        "id": "9-4PTTUL4AuP"
      },
      "source": [
        "##### 이미지 데이터 고해상도화(Super Resolution)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# super resolution을 진행할 모델 불러오기\n",
        "from transformers import Swin2SRImageProcessor, Swin2SRForImageSuperResolution\n",
        "pro_sr = Swin2SRImageProcessor.from_pretrained(\"./meta_data/super_resol/preprocessor\")\n",
        "model_sr = Swin2SRForImageSuperResolution.from_pretrained(\"./meta_data/super_resol/srwinsr.pt\")"
      ],
      "metadata": {
        "id": "uQ6izVeW1FPs"
      },
      "id": "uQ6izVeW1FPs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SR 진행된 이미지 리스트 반환 함수\n",
        "def image_list_with_sr(captions,dir,n,m):\n",
        "    imagelist=[]\n",
        "    for i in range(n,n+m):\n",
        "        path = dir+'/'+captions[i]['image']\n",
        "        image = Image.open(path)\n",
        "        image = super_reso(image) if image.size[0]<50 or image.size[1]<100 else image\n",
        "        imagelist.append(image)\n",
        "    return imagelist"
      ],
      "metadata": {
        "id": "2wmkoGl31F1K"
      },
      "id": "2wmkoGl31F1K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 개별 이미지 SR 진행 함수\n",
        "def super_reso(image):\n",
        "    inputs = pro_sr(image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model_sr(**inputs)\n",
        "\n",
        "    output = outputs.reconstruction.data.squeeze().cpu().float().clamp_(0, 1).numpy()\n",
        "    output = np.moveaxis(output, source=0, destination=-1)\n",
        "    output = (output * 255.0).round().astype(np.uint8)\n",
        "    return Image.fromarray(output)"
      ],
      "metadata": {
        "id": "Ws7l-P_21GHf"
      },
      "id": "Ws7l-P_21GHf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 조건에 따른 SR 적용된 이미지 리스트\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_sr.to(device)\n",
        "ImageList = image_list_with_sr(captions,'./meta_data/image_augmented',option['start'],option['num'])\n",
        "\n",
        "# SR 모델 CPU로 이동\n",
        "model_sr.to('cpu')\n",
        "\n",
        "# 25000장에서 약 3.5분 소요"
      ],
      "metadata": {
        "id": "U2GSB1NP1PGS"
      },
      "id": "U2GSB1NP1PGS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3p_hLjWF5oBp",
      "metadata": {
        "id": "3p_hLjWF5oBp"
      },
      "source": [
        "##### 이미지 데이터 정규화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6v9O-tgn0F2O",
      "metadata": {
        "id": "6v9O-tgn0F2O"
      },
      "outputs": [],
      "source": [
        "# 정규화 진행하는 preprocessor 폴더 임포트(import) 및 인스턴스화\n",
        "from transformers import BlipProcessor\n",
        "processor = BlipProcessor.from_pretrained('.meta_data/preprocessor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7642b9-fe49-4f55-9d26-01749c6e2643",
      "metadata": {
        "id": "3c7642b9-fe49-4f55-9d26-01749c6e2643"
      },
      "outputs": [],
      "source": [
        "# processor로 정규화된 데이터셋 클래스 작성\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 데이터 합치기 & 레이블 생성"
      ],
      "metadata": {
        "id": "4I6p-03l0z4v"
      },
      "id": "4I6p-03l0z4v"
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵션\n",
        "option = {\n",
        "    'start': 0,\n",
        "    'num': 25000,\n",
        "    'batch_size':16,\n",
        "}"
      ],
      "metadata": {
        "id": "GkbkYD_L2N1o"
      },
      "id": "GkbkYD_L2N1o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지와 캡션 매칭\n",
        "data = [{'text':captions[i]['label'],'image':ImageList[i]} for i in range(option['start'],option['start']+option['num'])]"
      ],
      "metadata": {
        "id": "EkVvs1xS20EW"
      },
      "id": "EkVvs1xS20EW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1gSVYpED5Jqz",
      "metadata": {
        "id": "1gSVYpED5Jqz"
      },
      "source": [
        "##### 훈련 & 평가 데이터셋 생성\n",
        "\n",
        "전체 데이터셋 중 **8:2**의 비율로 훈련:평가 데이터셋을 생성한다. 이때 배치 사이즈는 option 객체의 정보를 이용한다. (해당 프로젝트에서 진행한 배치 사이즈: 16개)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageCaptioningDataset(data[:int(0.8*option['num'])], processor)\n",
        "val_dataset = ImageCaptioningDataset(data[int(0.8*option['num']):], processor)\n",
        "train_dataloader = DataLoader(train_dataset,shuffle=False,batch_size = option['batch_size'])\n",
        "val_dataloader = DataLoader(val_dataset,shuffle=False,batch_size = option['batch_size'])"
      ],
      "metadata": {
        "id": "A0gOT4m209h-"
      },
      "id": "A0gOT4m209h-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader), len(val_dataloader)"
      ],
      "metadata": {
        "id": "uLUPHlbS05R_"
      },
      "id": "uLUPHlbS05R_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8ee2a19f-67ae-48d6-83c8-5eacff3813ca",
      "metadata": {
        "id": "8ee2a19f-67ae-48d6-83c8-5eacff3813ca"
      },
      "source": [
        "## **3. 학습 모델 훈련 (Train Model)**\n",
        "\n",
        "이미지 캡션을 위해 transformers에서 제공하는 pre-trained된 blip 모델을 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 구축\n",
        "from transformers import BlipForConditionalGeneration\n",
        "model = BlipForConditionalGeneration.from_pretrained(mode)"
      ],
      "metadata": {
        "id": "KVudfm7Z3b2R"
      },
      "id": "KVudfm7Z3b2R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습(train) 시 성능지표 확인 위한 코드"
      ],
      "metadata": {
        "id": "fCj-2b9L3cjP"
      },
      "id": "fCj-2b9L3cjP"
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델이 생성한 캡션을 json 파일로 저장해주는 함수\n",
        "def gen_captions(captions,filename):\n",
        "    gen = []\n",
        "    for i in range(len(captions)):\n",
        "        gen.append({'image_id': i+1, 'caption': captions[i]})\n",
        "\n",
        "    with open(filename,'w') as f:\n",
        "      json.dump(gen,f)"
      ],
      "metadata": {
        "id": "tH2yhYdi3csx"
      },
      "id": "tH2yhYdi3csx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성된 캡션을 기반으로, ground_truth 파일과 함께 성능지표를 계산해주는 함수\n",
        "def coco_caption_eval(annotation_file, results_file):\n",
        "\n",
        "    coco = COCO(annotation_file)\n",
        "    coco_result = coco.loadRes(results_file)\n",
        "\n",
        "    coco_eval = COCOEvalCap(coco, coco_result)\n",
        "    coco_eval.evaluate()\n",
        "\n",
        "    # print output evaluation scores\n",
        "    for metric, score in coco_eval.eval.items():\n",
        "        print(f'{metric}: {score:.3f}')\n",
        "\n",
        "    return coco_eval"
      ],
      "metadata": {
        "id": "8Ip7plhc3c0j"
      },
      "id": "8Ip7plhc3c0j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_cpath = './meta_data/output/val' # validation 위해 모델이 생성한 캡션 파일 저장하는 경로\n",
        "val_rpath = './meta_data/annotations/gt_captions.json' # 성능지표 계산 시 참조하는 ground-truth 캡션 파일 경로\n",
        "train_hist=[] # loss 값 확인 위한 리스트\n",
        "val_hist=[] # loss 값 확인 위한 리스트"
      ],
      "metadata": {
        "id": "GZ4jqnIJ3dDx"
      },
      "id": "GZ4jqnIJ3dDx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fine-tuning 대상 layer 정하기\n",
        "\n",
        "- 모델 구조 살펴보기\n",
        "\n",
        "- 모델 layer 중 동결(frozen)할 layer와 학습가능(trainable)한 layer 구분하기\n",
        "  - 해당 프로젝트 실험 결과, 전층(all layer)를 작은 학습률(lr: 1e-6)로 fine-tuning한 버전이 가장 높은 성능을 보였음\n",
        "\n"
      ],
      "metadata": {
        "id": "PxPBCLcd40Ey"
      },
      "id": "PxPBCLcd40Ey"
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 구조 살펴보기\n",
        "print(model)"
      ],
      "metadata": {
        "id": "qJaUT7FJ5PiI"
      },
      "id": "qJaUT7FJ5PiI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요에 따라, 동결할 레이어 동결시키기\n",
        "'''n = 0\n",
        "for l in model.text_decoder.bert.encoder.layer:\n",
        "  n += 1\n",
        "  if n >= 12:\n",
        "    l.crossattention.requires_grad_()\n",
        "  else:\n",
        "    continue'''"
      ],
      "metadata": {
        "id": "OIWQwNUD5SO4"
      },
      "id": "OIWQwNUD5SO4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동결, 학습가능 layer 확인\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, \"is trainable\")\n",
        "    else:\n",
        "        print(name, \"is frozen\")"
      ],
      "metadata": {
        "id": "EdLk26mp5uIW"
      },
      "id": "EdLk26mp5uIW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0d590e79-1916-4700-b50d-63322b892487",
      "metadata": {
        "id": "0d590e79-1916-4700-b50d-63322b892487"
      },
      "source": [
        "### 모델 학습\n",
        "- 전이학습을 통한 fine-tuning\n",
        "  - 앞서 준비된 사전학습 모델을 준비해준 입력 데이터와 레이블 데이터로 학습시킨다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-6\n",
        "E = 30\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=E)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(E):\n",
        "    model.train()\n",
        "    Loss = 0\n",
        "\n",
        "    #train_caption = [] -> train 시 성능지표를 보기 위한 리스트 (선택사항)\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "        input_ids = batch.pop(\"input_ids\").to(device)\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "        outputs = model(input_ids=input_ids,pixel_values=pixel_values, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # train 시 성능지표를 보기 위한 작업 (선택사항)\n",
        "        #with torch.no_grad():\n",
        "        #  model.eval()\n",
        "        #  train_caption+=processor.batch_decode(model.generate(pixel_values=pixel_values,max_length = 100),skip_special_tokens=True)\n",
        "        #train_hist2.append(loss.tolist())\n",
        "\n",
        "        Loss+=loss.tolist()\n",
        "\n",
        "    train_hist.append(Loss/len(train_dataloader)) # train loss 확인 위한 코드\n",
        "\n",
        "\n",
        "    # 에폭마다 validation 진행\n",
        "    val = 0\n",
        "    val_caption =[]\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, batch in enumerate(val_dataloader):\n",
        "            input_ids = batch.pop(\"input_ids\").to(device)\n",
        "            pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "            outputs = model(input_ids=input_ids,pixel_values=pixel_values, labels=input_ids)\n",
        "\n",
        "            # 성능을 보기위한 작업: 1. 캡션 생성해서 리스트에 담아두기 2. Val loss 확인하기\n",
        "            val_caption+=processor.batch_decode(model.generate(pixel_values=pixel_values,max_length = 300),skip_special_tokens=True)\n",
        "            val+=outputs.loss.tolist()\n",
        "\n",
        "    val_hist.append(val/len(val_dataloader))\n",
        "\n",
        "    # checkpoint model 저장\n",
        "    if val_hist[-1]==min(val_hist):\n",
        "        torch.save(model,'./meta_data/checkpoint_best.pt')\n",
        "\n",
        "    #Epoch 출력\n",
        "    print(\"Epoch {}회차 - val_Loss:{}, \".format(epoch+1,val/313))\n",
        "\n",
        "    # epoch의 caption들 저장 및 성능 출력을 위한 코드\n",
        "    # meata_data 하위 폴더에 에폭별 Val 데이터셋에 대한 생성 캡션을 json 파일로 저장하기 -> 이후 성능지표 계산을 위해\n",
        "    gen_captions(val_caption,val_cpath+'/'+str(epoch+1)+'.json')\n",
        "\n",
        "    # train 시 성능지표 확인 위한 코드 (선택사항)\n",
        "    #gen_captions(train_caption,train_cpath+'/'+str(epoch+1)+'.json')"
      ],
      "metadata": {
        "id": "nLNWbDiC6Sog"
      },
      "id": "nLNWbDiC6Sog",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d2448471-996d-425e-8130-aca498eb8e5f",
      "metadata": {
        "id": "d2448471-996d-425e-8130-aca498eb8e5f"
      },
      "source": [
        "### 모델 평가 (Evaluate Model)\n",
        "\n",
        "- 기계번역, 캡션 task 관련 성능지표 계산\n",
        "- BLEU, METEOR, ROUGE-L, CIDEr, SPICE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = []\n",
        "for i in range(30):\n",
        "    val.append(coco_caption_eval(\"./meta_data/annotaions/gt_captions.json\",f'.meta_data/output/val/{i+1}.json').eval.items())"
      ],
      "metadata": {
        "id": "ubCbq0Ur9gmK"
      },
      "id": "ubCbq0Ur9gmK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 json 파일로 저장\n",
        "with open('./meta_data/ouput/score.json','w') as f:\n",
        "  json.dump(val,f)"
      ],
      "metadata": {
        "id": "5x1as03t-Nip"
      },
      "id": "5x1as03t-Nip",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d46de9ff-fdb3-40f0-a40e-ddee8afbfd46",
      "metadata": {
        "id": "d46de9ff-fdb3-40f0-a40e-ddee8afbfd46"
      },
      "source": [
        "## **4. 추론 (Inference)**\n",
        "\n",
        "훈련시킨 모델을 직접 사용해보고자 한다. 잘 훈련된 모델이라면 처음 보는 cctv 영상 속 사람 객체 이미지를 인풋으로 줬을 때, 외양을 묘사하는 정확한 캡션을 생성할 것이다.\n",
        "\n",
        "- 이미지 불러오기\n",
        "- 조건에 따른 SR 적용\n",
        "- 데이터 변환 및 결과 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9_QmyD1J9fR6",
      "metadata": {
        "id": "9_QmyD1J9fR6"
      },
      "source": [
        "##### 이미지 불러오기\n",
        "미리 준비해둔 test 이미지 파일을 받아 추론을 진행해보고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91bfb4aa-7b4a-40bd-8f8a-6d018b6edf9c",
      "metadata": {
        "id": "91bfb4aa-7b4a-40bd-8f8a-6d018b6edf9c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# dataset.zip 파일을 dataset 폴더에 압축을 풀어준다.\n",
        "zip_source_path = './inference_image_dataset.zip'\n",
        "zip_target_path = './meta_data'\n",
        "\n",
        "extract_zip_file = zipfile.ZipFile(zip_source_path)\n",
        "extract_zip_file.extractall(zip_target_path)\n",
        "\n",
        "extract_zip_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AzpDECSy-5gb",
      "metadata": {
        "id": "AzpDECSy-5gb"
      },
      "source": [
        "##### 조건에 따른 SR 적용\n",
        "- 이미지 데이터를 읽어온다.\n",
        "- SR를 실시한다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 데이터 SR한 리스트 반환 함수\n",
        "def image_list_with_sr_inf(dir,n,m):\n",
        "    imagelist=[]\n",
        "    for i in range(n,n+m):\n",
        "        path = dir+'/'+os.listdir(dir)[i]\n",
        "        image = Image.open(path)\n",
        "        image = super_reso(image) if image.size[0]<50 or image.size[1]<100 else image\n",
        "        imagelist.append(image)\n",
        "    return imagelist"
      ],
      "metadata": {
        "id": "WQHHtjoMB_8m"
      },
      "id": "WQHHtjoMB_8m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')\n",
        "\n",
        "model_sr.to(device)\n",
        "ImageList = image_list_with_sr_inf('.meta_data/inference_image_dataset',0,10)\n",
        "\n",
        "model_sr.to('cpu') # 5000장에서 약 2분 소요"
      ],
      "metadata": {
        "id": "UVhEDtFKA-EW"
      },
      "id": "UVhEDtFKA-EW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 데이터 변환 및 결과 확인\n",
        "- 데이터 변환\n",
        "- 각 사진마다 생성된 caption 확인"
      ],
      "metadata": {
        "id": "oo6qNtJeCLIX"
      },
      "id": "oo6qNtJeCLIX"
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values =  processor.image_processor(images=ImageList, return_tensors=\"pt\").pixel_values # 데이터 변환\n",
        "model.to(device)\n",
        "outputs=processor.batch_decode(model.generate(pixel_values=pixel_values.to(device),max_length = 70),skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "wXw7fHoHBcNV"
      },
      "id": "wXw7fHoHBcNV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(outputs[i])\n",
        "    ImageList[i].show()\n",
        "    print()"
      ],
      "metadata": {
        "id": "hB0LWq9dBcW6"
      },
      "id": "hB0LWq9dBcW6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4c067e6a-8187-4727-b55a-436b7d55b780",
        "9-4PTTUL4AuP",
        "3p_hLjWF5oBp",
        "4I6p-03l0z4v",
        "1gSVYpED5Jqz",
        "fCj-2b9L3cjP",
        "PxPBCLcd40Ey",
        "0d590e79-1916-4700-b50d-63322b892487",
        "d2448471-996d-425e-8130-aca498eb8e5f",
        "oo6qNtJeCLIX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}